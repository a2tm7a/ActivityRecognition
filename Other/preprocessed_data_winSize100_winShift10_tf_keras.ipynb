{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((218288, 1200), (218288,), (93552, 1200), (93552,))\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "from keras.optimizers import SGD\n",
    "import logging\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "h5f = h5py.File('preprocessed_data_winSize100_winShift10.h5','r')\n",
    "training_data = h5f['training_data'][:]\n",
    "training_output = h5f['training_output'][:]\n",
    "testing_data = h5f['testing_data'][:]\n",
    "testing_output = h5f['testing_output'][:]\n",
    "\n",
    "h5f = h5py.File('preprocessed_data_winSize100_winShift10_mean_sd.h5','r')\n",
    "mean = h5f['mean'][:]\n",
    "sd = h5f['sd'][:]\n",
    "h5f.close()\n",
    "\n",
    "print(training_data.shape,training_output.shape,testing_data.shape,testing_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating training\n",
      "Deleting training\n",
      "Shuffling training\n",
      "Concatenating testing\n",
      "Deleting testing\n",
      "Shuffling Testing\n",
      "Getting back train data and output\n",
      "Deleting training\n",
      "Getting back test data and output\n",
      "Deleting testing\n",
      "((218288, 1200), (218288,), (93552, 1200), (93552,))\n"
     ]
    }
   ],
   "source": [
    "print(\"Concatenating training\")\n",
    "training = np.concatenate([training_data, training_output.reshape(training_output.shape[0],1)], axis=1)\n",
    "print(\"Deleting training\")\n",
    "del training_data\n",
    "del training_output\n",
    "print(\"Shuffling training\")\n",
    "np.random.shuffle(training)\n",
    "\n",
    "print(\"Concatenating testing\")\n",
    "testing = np.concatenate([testing_data, testing_output.reshape(testing_output.shape[0],1)], axis=1)\n",
    "print(\"Deleting testing\")\n",
    "del testing_data\n",
    "del testing_output\n",
    "print(\"Shuffling Testing\")\n",
    "np.random.shuffle(testing)\n",
    "\n",
    "print(\"Getting back train data and output\")\n",
    "training_data=training[:,:-1]\n",
    "training_output=training[:,-1]\n",
    "print(\"Deleting training\")\n",
    "del training\n",
    "\n",
    "print(\"Getting back test data and output\")\n",
    "testing_data=testing[:,:-1]\n",
    "testing_output=testing[:,-1]\n",
    "print(\"Deleting testing\")\n",
    "del testing\n",
    "\n",
    "print(training_data.shape,training_output.shape,testing_data.shape,testing_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((218288, 1200), (218288,))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "testing_data=testing_data-mean\n",
    "training_data=training_data-sd\n",
    "\n",
    "print(training_data.shape,training_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/utils/np_utils.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y[i, y[i]] = 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218288, 7)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "nb_classes = 7\n",
    "\n",
    "training_output = np_utils.to_categorical(training_output, nb_classes)\n",
    "testing_output = np_utils.to_categorical(testing_output, nb_classes)\n",
    "print(training_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218288, 1200)\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "nb_epoch = 10\n",
    "nb_neg_cycles = 3\n",
    "\n",
    "print(training_data.shape)\n",
    "\n",
    "logging.basicConfig(filename= \"./WSDM_Keras_batch_size\"+\"(\"+str(batch_size)+\")\"+\".log\", level=logging.INFO, format='%(message)s')\n",
    "logging.info(\"Parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_8 (Dense)                  (None, 256)           307456      dense_input_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 256)           0           dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 7)             1799        activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 7)             0           dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 309255\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(256, input_shape=(training_data.shape[1],)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch = 1\n",
    "\n",
    "final_acc=0.0\n",
    "lr = 0.0001\n",
    "weights=[]\n",
    "first_run=True\n",
    "flag=True\n",
    "iter_count = 1\n",
    "count_neg_iter=0\n",
    "\n",
    "\n",
    "#The flag to keep the loop running\n",
    "run_flag=True\n",
    "weights=[]\n",
    "\n",
    "#Check if it is the first iteration\n",
    "first_iter=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Learning Rate : ', 0.0001)\n",
      "Train on 218288 samples, validate on 93552 samples\n",
      "Epoch 1/1\n",
      "218288/218288 [==============================] - 7s - loss: 1.6717 - acc: 0.7800 - val_loss: 1.8902 - val_acc: 0.7590\n",
      "('Test score:', 1.8902443540143175)\n",
      "('Test accuracy:', 0.7590064805492247)\n",
      "Updating the weights\n",
      "('Learning Rate : ', 0.0001)\n",
      "Train on 218288 samples, validate on 93552 samples\n",
      "Epoch 1/1\n",
      "218288/218288 [==============================] - 7s - loss: 1.2395 - acc: 0.8037 - val_loss: 1.7775 - val_acc: 0.7585\n",
      "('Test score:', 1.7774774218412961)\n",
      "('Test accuracy:', 0.7585437905386776)\n",
      "Reducing the learning rate by half\n",
      "('Learning Rate : ', 5e-05)\n",
      "Train on 218288 samples, validate on 93552 samples\n",
      "Epoch 1/1\n",
      "218288/218288 [==============================] - 7s - loss: 1.2869 - acc: 0.8013 - val_loss: 1.8271 - val_acc: 0.7576\n",
      "('Test score:', 1.8271492387120165)\n",
      "('Test accuracy:', 0.75761840641957301)\n",
      "Reducing the learning rate by half\n",
      "('Learning Rate : ', 2.5e-05)\n",
      "Train on 218288 samples, validate on 93552 samples\n",
      "Epoch 1/1\n",
      "218288/218288 [==============================] - 7s - loss: 1.3152 - acc: 0.7996 - val_loss: 1.8594 - val_acc: 0.7586\n",
      "('Test score:', 1.8593800852436153)\n",
      "('Test accuracy:', 0.75859418342679064)\n",
      "Reducing the learning rate by half\n",
      "('Learning Rate : ', 1.25e-05)\n",
      "Train on 218288 samples, validate on 93552 samples\n",
      "Epoch 1/1\n",
      "218288/218288 [==============================] - 7s - loss: 1.3305 - acc: 0.7987 - val_loss: 1.8764 - val_acc: 0.7592\n",
      "('Test score:', 1.876445818624273)\n",
      "('Test accuracy:', 0.75920194178457767)\n",
      "Reducing the learning rate by half\n"
     ]
    }
   ],
   "source": [
    "while run_flag:\n",
    "\n",
    "    #Give ramdom weights in first iteration and previous weights to other\n",
    "    if first_iter:\n",
    "        first_iter=False\n",
    "    else:\n",
    "        model.set_weights(np.asarray(weights))\n",
    "\n",
    "\n",
    "    logging.info(\"\\n\\nIteration:\"+str(iter_count))\n",
    "    print (\"Learning Rate : \",lr)\n",
    "    logging.info(\"Learning Rate: \"+str(lr))\n",
    "    \n",
    "    sgd = SGD(lr=lr)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(training_data, training_output, batch_size=batch_size, nb_epoch=1,\n",
    "              verbose=1, validation_data=(testing_data, testing_output))\n",
    "    score = model.evaluate(testing_data, testing_output, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    current_acc = score[1]\n",
    "\n",
    "    logging.info(current_acc)\n",
    "\n",
    "    if(current_acc - final_acc > 0.0005):        \n",
    "        iter_count = iter_count + 1\n",
    "\n",
    "        #Update the weights if the accuracy is greater than .001\n",
    "        weights=model.get_weights()\n",
    "        print (\"Updating the weights\")\n",
    "        logging.info(\"Updating the weights\")\n",
    "        #Updating the final accuracy\n",
    "        final_acc=current_acc\n",
    "        #Setting the count to 0 again so that the loop doesn't stop before reducing the learning rate n times consecutivly\n",
    "        count_neg_iter = 0\n",
    "\n",
    "    else:\n",
    "        #If the difference is not greater than 0.005 reduce the learning rate\n",
    "        lr=lr/2.0\n",
    "        print (\"Reducing the learning rate by half\")\n",
    "        logging.info(\"Reducing the learning rate by half\")\n",
    "        count_neg_iter = count_neg_iter + 1\n",
    "        \n",
    "        #If the learning rate is reduced consecutively for nb_neg_cycles times then the loop should stop\n",
    "        if(count_neg_iter>nb_neg_cycles):\n",
    "            run_flag=False\n",
    "            model.set_weights(np.asarray(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Final accuracy:', 0.79569806586947833)\n"
     ]
    }
   ],
   "source": [
    "print('Final accuracy:', final_acc)\n",
    "\n",
    "logging.info(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
